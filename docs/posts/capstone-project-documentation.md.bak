---
weight: 10
title: "Capstone Project Documentation"
date: 2023-06-26T21:29:01+08:00
description: "Documentation of the project and results"
tags: ["bike", "model", "prediction","machine learning"]
type: post
showTableOfContents: true
---

## What's Bicing ?

The new Bicing service includes more territorial coverage, an increase in the number of bicycles, mixed stations for conventional and electric bicycles, new and improved types of stations and bicycles (safety, anchorage, comfort), extended schedules and much more!

## Goal

There are two main objective in this project:

- Predict the number of free docks given the historical data (Docks Availability Percent).

- Explore new places where stations are needed.

- Explore how different events affect availability.


## Get the Data

According with the project we have the next sources:

- The bicing stations status
- The bicing stations information
- The weather of the city of Barcelona

The Bicing stations status and information of the city of Barcelona were downloaded from [Open Data BCN](https://opendata-ajuntament.barcelona.cat) and the weather information to join with bicing stations were downloaded from [Meteo Cat](https://www.meteo.cat).

### a. Download the Data

The following script was used to download the data from [Open Data BCN](https://opendata-ajuntament.barcelona.cat) by year and month:

```bash
import os

i2m = list(zip(range(1,13), ['Gener','Febrer','Marc','Abril','Maig','Juny','Juliol','Agost','Setembre','Octubre','Novembre','Desembre']))
for year in [2022, 2021, 2020, 2019]:
    for month, month_name in i2m:        
        os.system(f"wget 'https://opendata-ajuntament.barcelona.cat/resources/bcn/BicingBCN/{year}_{month:02d}_{month_name}_BicingNou_ESTACIONS.7z'")
        os.system(f"7z x '{year}_{month:02d}_{month_name}_BicingNou_ESTACIONS.7z'")
        os.system(f"rm '{year}_{month:02d}_{month_name}_BicingNou_ESTACIONS.7z'")

```

The following script was used to download the data from [Meteo Cat](https://www.meteo.cat) and integrate it into the station availability patterns

```python
# CODE TO RETRIEVE DATA FROM THE WEATHER API AND STORE IT IN A CSV FILE
# FROM 2019/01/01 TO 2023/03/31

import requests
import time
import datetime as dt
import csv 

key = 'enJH8FUX2z5Ar7NSCJvYI8pAIuDW0XDV9nbSkEMj'

start_date = dt.date(2019, 1, 1)
end_date = dt.date(2023, 3, 31)

assert start_date < end_date, 'Start date must be before end date'

delta = dt.timedelta(days=1)

freq = 1/20

day = start_date

columns = ['timestamp', 'mm_precip', 'temperature']

with open('weather_data/weather.csv', mode = 'w', newline='') as csvfile:
    writer = csv.DictWriter(csvfile, fieldnames=columns)
    writer.writeheader()

    while day <= end_date:

        day_string = day.strftime('%Y/%m/%d')

        url = 'https://api.meteo.cat/xema/v1' + '/estacions/mesurades/X8/' + day_string 

        print(day_string)

        headers = {'Accept': 'application/json', 'X-API-KEY': key}

        data = requests.get(url, headers=headers).json()[0]

        # the codi variables, 35 and 32, correspond to precipitation and temperature respectively. The json file retrieved
        # contains information on many more variables, but we are only interested in these two.

        precipitation = [data['variables'][i]['lectures'] for i in range(len(data['variables'])) if data['variables'][i]['codi'] == 35][0]
        temperature = [data['variables'][i]['lectures'] for i in range(len(data['variables'])) if data['variables'][i]['codi'] == 32][0]

        date_variables = [{'timestamp':int(dt.datetime.strptime(d['data'], '%Y-%m-%dT%H:%MZ').timestamp()), 'mm_precip':d['valor']} for d in precipitation]

        for i in range(len(date_variables)):
            date_variables[i]['temperature'] = temperature[i]['valor']

        writer.writerows(date_variables)

        day += delta

        time.sleep(freq)
```

### b. Consolidate the Data

The consolidation of the Data was built in Tableau using Tableau Prep which give us more flexibility to join, filter and build the next structure according with our metadata-sample-submission.csv

#### Metadata Sample Submission

![Metadata Sample Submission](/capstone-project/metadata-sample-submission.png)

## Discover and visualize the data to gain insights

Data consolidation, visualization and analysis with Tableau

### a. Tableau Workflow

The first flow in Tableau Prep let you consolidate the bicing station status files of all years and create new columns like year, month, day and hour. 
The flow has two sub-flows because is necessary applied the same process to bicing station 2023 files. Both flows create new files with a hyper format which is easier to manage and control the data.

![First Tableau flow](/capstone-project/first-tableau-flow.png)

The second flow in Tableau Prep get the hyper files of bicing station status (2019-2022) to join with the last bicing station information (March, 2023) to get the extra information like longitude, latitude, name, capacity and postcode.

![Second Tableau flow](/capstone-project/second-tableau-flow.png)

The same process is applied to the bicing station status 2023 but filtering docks_availability and bike_availability. These fields will be evaluated and predicted by the models.

![Third Tableau flow](/capstone-project/third-tableau-flow.png)

Finally, the last flow transform the input data of the preview flow using a aggregation to the level year, month, day and hour calculating average of the rest of fields and creating four additional fields with the percent of docks availability in the four hours before.

![Fourth Tableau flow](/capstone-project/fourth-tableau-flow.png)

Before training and testing, the data is analyzed to identify patterns, outliers and to visualize relevant features.

### b. Analysis in Tableau

Visual analysis of data constructed based on the final output to identify outliers, COVID behaviors and relevant characteristics before training and testing with machine learning models

![Bicing Data Analysis](/capstone-project/bicing-data-analysis.png)

With all the consolidate information we start to clean and processes the data to prepare our dataframes of training and testings.

## Prepare the data for Machine Learning algorithms

### a. Importation of data in Colab

We load the dataframes of bicing station (validation and training), weather and testing.

```python
station_dataframe = dd.read_csv('/content/drive/MyDrive/CapstoneProject/data_bicing_joined_HX_23.csv', assume_missing=True, delimiter=';')

weather_dataframe = dd.read_csv('/content/drive/MyDrive/CapstoneProject/weather.csv', assume_missing=True, delimiter=',')

data_2_predict = dd.read_csv('/content/drive/MyDrive/CapstoneProject/metadata_sample_submission.csv', delimiter=',')

```
### b. Data Cleaning and Filtering

- Initial cleaning to go from the format that the Tableau merger outputs to the one desired for the model.
- The objective of this first step is to leave the training data in the same format as the submission one.
- Drop all rows with out of service stations and year 2020, to avoid COVID effects on our training data.
- Select the features with the highest relevance (station_id, lat, lon, year, month, day, hour, ctx-4, ctx-3, ctx-2, ctx-1, percentage).
- Split the data into training and validation.
- Filter only the data with status "IN_SERVICE".

### c. Processing pipelines

Apart from the preprocessing done using tableau to create the initial dataframe, everything else will be done using sklearn pipelines. 

Creation of the 4 pipelines:
- train_preparator: 
  - to prepare the training data, not used for the submission data as we already have location data.
  - we have five transformers. The first merge the data with weather dataframe and include the creation of the datatime column.
  - the second transform add new columns of time.
  - the third and fourth transformer filter the hours to avoid overlapping and normalize datetime columnes like month, day, hour and year.
  
- submisison_preparator and val_preparator: 
  - to prepare the submission data, not used for the training data as we don't have location data. Not a problem since we are not fitting to any data, just transforming with data we already have
  - we keep the same transformers of the previous pipeline because we want to have the same structure with the data.
  
- scaling_pipeline: 
  - to fill and scale certain column in the data, used for both training and submission data. In this case, we are fitting to the training data, and transforming both training and submission data


### d. Classes and Functions

Each pipeline is supported by a set of functions and classes to structure the data and features. The functions are included like parameters inside of the classes.

#### Functions

- extra_time_info: add new columns of time like is_weekend, timeframe1, timeframe2, timeframe3, timeframe4, timeframe5 to group the hours
- hour_selector: filter a range of hours to avoid overlapping with the columns with the same information of one to four hours before
- time_norm: Time normalizer function to normalize the time columns to a 0-1 scale with periodicity. This is done by applying a sin and cos transformation to the columns
- weather_prep: 
  - weather dataframe preparator to leave it in the desired format to merge with the station dataframe.
  - We convert the half hourly data to hourly data by averaging temperature and summing precipitation
  - We also add a column with the datetime in order to merge the dataframes
- weather_merge: Merge the main dataframe with weather dataframe and split datetime column in year, month, day and hour

#### Classes

- hour_selector_transformer: class to encapsulate the range of hours.
- station_loc_transformer: class to join and encapsulate the latitude and longitude.
- station_id_dropper: the class use station id dropper function to drop the station_id column from the dataset at the end of the pipeline. The prediction will be done on the location of the stations, not on the id.
- time_norm_transformer: class to encapsulate a time columns and normalize them using time_norm function.
- weather_merge_transformer: class to encapsulate the weather data and merge with main dataframe.


### e. Prepare all data and load data that has been saved

The following code load the data has been saved in the prepped_data folder, so there is no need to run this code again. We will just load it from our folder to work with the models

We have three datasets. The first dataset is for training purposes, the second dataset considers only a specific range of hours to avoid overlapping data with the columns, and the last dataset is our test dataset, which is loaded on Kaggle.


- Training Dataset

  ![X-Train dataset](/capstone-project/x-train.png)

#### Validation Dataset (5 hr)

![X-Val dataset](/capstone-project/x-val.png)

#### Validation Dataset with all hours (24 hr)

![X-Val24 dataset](/capstone-project/x-val24.png)

#### Submission Dataset (Testing Dataset)

![X-Submission dataset](/capstone-project/x-submission.png)

## Model Selection and Training

For this project we will use 2 different models. The first one is an xgboost regressor, that has been chosen following other similar projects to this one and that has outperformed with less training times other similar ensemble models. Then, a neural network containing LSTM layers is used to capture the temporal features and their ordering.

### a. XGBregressor

We will first use a data subset to perform a grid search on the parameter space to find the ones that fit the best our xgboost model.
 
#### RandomizedSearchCV and GridSearchCV

We apply both techniques to find the best parameters for our model after transforming our data using pipelines. We use an XGBRegressor and a set of parameters associated with it to evaluate the optimal parameters for training and prediction.

```python
space = {
    'learning_rate': [0.1, 0.01, 0.001],
    'max_depth': [3, 5, 7],
    'subsample': [0.8, 0.9, 1.0],
    'reg_lambda': [0.1, 1.0, 10.0],
    'reg_alpha': [0, 0.1, 1.0],
    'n_estimators': [30, 50, 100, 200]
}

xgb_model = XGBRegressor(objective='reg:squarederror', random_state = 40)
grid_search = GridSearchCV(estimator=xgb_model, param_grid=space, scoring='neg_mean_squared_error', cv=2, verbose=2)

grid_search.fit(gs_subset, y_subset)

```

Best parameters result

![Best Parameters GridSearch](/capstone-project/grdsearch-best-parameters.png)

Now that we have the best parameters for our subset of data we train the model. We also use the validation dataset to monitor the performance of the model during training to look for over-fitting curves

```python

xgb_model = XGBRegressor(objective='reg:squarederror', colsample_bytree=0.8, gamma=0.1, n_jobs=-1, random_state=123, eval_metric = 'rmse', early_stopping_rounds = 30)

with open('/content/drive/MyDrive/CapstoneProject/xgboost models/best_params.json', 'r') as f:
  best_params = json.load(f)

xgb_model.set_params(**best_params)

evalset = [(X_train, y_train), (X_val, y_val), (X_val24, y_val24)]

xgb_model.fit(X_train, y_train, eval_set = evalset)

y_pred = xgb_model.predict(X_val)

rmse = np.sqrt(mean_squared_error(y_true = y_val, y_pred = y_pred))

print('rmse:', rmse)

```

Once we have trained the model, we are interested in knowing how the loss evolved during training, as well as what variables were the most relevant to predict the percentage of available docks

#### Loss Function Graphic

![Loss Function graphic](/capstone-project/loss-function-xgboost.png)

#### Most Relevant Features

![Feature Importances Graphic](/capstone-project/feature-importance-xgboost.png)

The feature important plot is really interesting. From it we can conclude that:

- A expected, the previous hours percentage gives a lot of info about how many docks will be available in the following hours.
- The location of the station are the next most relevant features. The usage of the service is not homogeneous across the city and this impacts to availability of docks
- timeframe2, corresponding to peak hours in the morning has quite a bit of weight. The stations network probably has the most activity in these hours
- the hour also affects quite a bit the final %, as expected --> Since we are only training with
- The weather does not even appear in the graph. We expected way more weight from those variables, especially rain. We guess that since Barcelona is not a very rainy city, the model does not see many registers with rain and so it end up ignoring completely the variable

### b. LSTM

Apart from our xgbregressor model, and since our data contains historical data, we have also tried to study the prediction problem as a time-series analysis by working with long short-term memory layers on a neural network to try and exploit the temporal ordering of the information. Since not all the variables are historical, that have built a time series with just the one that are historical context, and we have treated the rest with a fully connected network. By using this approach our intention is to capture not only the static context of the data, but also the past that leads to each station's state.

We construct a neural network by separating it into two parts, one for the time series data and one for the static data. The time series data is fed into an LSTM layer, the static data is fed into a dense layer


#### Neural Network Structure

![Neural Network](/capstone-project/neural-network-structure.png)

#### Neural Network Results

![Neural Network Results](/capstone-project/neural-network-results.png)


We get recall similar results to that of the xgb model --> Further improvements should come from the hand of a better preproccesing probably, working with outliers and incorrect data in a more detailed way

## Heatmap and Buffer Analysis

This section consists in two parts:

- Creation of a dinamic heat map of dock availability that can be changed based on the month and year parameter. This will allow us to understand the pattern of bicycle usage across the city.

- Performance of a buffer analysis. Here we are going to search for areas that are near the bike lanes but do not have a nearby bike station to detect potential zones where new bike stations could be installed. The buffer has been set in 500 meters based on the size of the city and the number of stations.

### a. Libraries

```python
import folium
from folium.plugins import HeatMap
import pandas as pd
import json
import numpy as np
import dask.dataframe as dd
import dash
from dash.dependencies import Input, Output
from dash import dcc as dcc
from dash import html as html
import base64
import os
from flask import Flask
from io import BytesIO
from jupyter_dash import JupyterDash
import geopandas as gpd
from shapely.geometry import LineString
from shapely.geometry import Point, Polygon
import geopy.distance
import math

```
### b. Heatmap

The code is designed to generate and display the dinamic heatmap that represents the average availability of bike docks across the city, given a specific month and year.First, lets create a joined dataframe that will be used for the heatmap analysis


![Heatmap Dataframe](/capstone-project/heatmap-dataframe.png)

Now we are going to calculate the average percentage of bike availability at each station using the historical data. then we will generate a heatmap based on this data, with coordinates of each station and a color intensity indicating the level of bike availability. This heatmap is overlaid onto the city map created using Folium, providing a clear visualization of bike availability in different areas of the city.

The resulting map is saved as an HTML file in a temporary location and then the interface powered by Dash includes dropdown menus for selecting the month and year of interest. The map updates dynamically based on these selections, providing a tool for exploring dock availability across different times.

![Heatmap](/capstone-project/bicing-heatmap-capture.png)

![Heatmap2](/capstone-project/bicing-heatmap-capture2.png)


Remarks: If we compare the different periods, we can see that in general, the downtown is the area where there is a major use of bicycle and therefore less availability.

### c. Buffer Analysis

This produces a heatmap representing the average availability of bikes at different stations in march 2023 including the buffer analysis around bike lanes.

First, we define a function function to calculate the geographical distance between two points on Earth given their latitude and longitude. This function will be later used in the buffer analysis.

Next, we compute the average availability of bikes at each station for march 2023. This data is used to create a list of tuples where each tuple contains the latitude, longitude and availability of a bike station. The bike availability is subtracted from 1 to display lack of availability in the heatmap.

A base map of the city is created using Folium and the bike lanes data is loaded from a GeoJSON file and added to the map.The heatmap is added to the map using the previously computed bike station data.

The script then performs the buffer analysis. It iterates through each bike lane segment and checks if it's more than 500 meters away from any bike station. If it is, a marker is added to that point on the map. This analysis helps identify undeserved areas where additional bike stations might be needed. Then is saved as an html.

The output is the following:

[Buffer Analysis](/capstone-project/temp_map_no_mark23.html)

Remarks: With this buffer analysis we can see that there are several areas in the city that might need a station, such as Zona franca, the port of Barcelona or near Sant Adria de Besos.

